{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3(4).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va718DM8Mcx6"
      },
      "source": [
        "# Siamese Network Mnist Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFlfGOagWd0J"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "class DATA():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.batch_size = 128\n",
        "        self.transform = None\n",
        "        self.data_index = 0\n",
        "        self.dataset = None\n",
        "        self.train_dataloader = None\n",
        "        self.test_dataloader = None\n",
        "        #self.test_data = None\n",
        "\n",
        "    def read(self):\n",
        "        trans = transforms.Compose([transforms.ToTensor()])\n",
        "        self.dataset = MNIST(root=\"~/torch_datasets\", train = True, transform = trans, download=True)\n",
        "        self.dataset_test = MNIST(root=\"~/torch_datasets\", train = False, transform = trans, download=True)\n",
        "        self.train_dataloader = DataLoader(self.dataset, self.batch_size, shuffle = True)\n",
        "        self.test_dataloader = DataLoader(self.dataset_test, self.batch_size, shuffle = True)\n",
        "\n",
        "    def read_test(self):\n",
        "      pass\n",
        "        # dataset = input_data.read_data_sets(root=\"~/torch_datasets\", one_hot = False)   \n",
        "        # test_data_x = dataset.test.images\n",
        "        # test_data_y = dataset.test.labels\n",
        "        # return test_data_x, test_data_y\n",
        "    \n",
        "    def generate_batch(self):\n",
        "      train_iter = iter(self.train_dataloader)\n",
        "      input_1, label_1 = next(train_iter)\n",
        "      input_2, label_2 = next(train_iter)\n",
        "      input_1 = input_1.reshape(input_1.size()[0], -1)\n",
        "      input_2 = input_2.reshape(input_2.size()[0], -1)\n",
        "      np_label_1 = label_1.numpy()\n",
        "      np_label_2 = label_2.numpy()\n",
        "      label = (np_label_1 == np_label_2).astype('float32')\n",
        "      return input_1, input_2, label\n",
        "\n",
        "    def generate_batch_test(self):\n",
        "      train_iter = iter(self.test_dataloader)\n",
        "      input_1, label_1 = next(train_iter)\n",
        "      input_2, label_2 = next(train_iter)\n",
        "      input_1 = input_1.reshape(input_1.size()[0], -1)\n",
        "      input_2 = input_2.reshape(input_2.size()[0], -1)\n",
        "      np_label_1 = label_1.numpy()\n",
        "      np_label_2 = label_2.numpy()\n",
        "      label = (np_label_1 == np_label_2).astype('float32')\n",
        "      return input_1, input_2, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTxG20JfY2Ow"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "    \n",
        "\n",
        "class MODEL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MODEL, self).__init__()\n",
        "        self.HiddenLayer_1 = nn.Linear(28*28, 1024)\n",
        "        self.HiddenLayer_2 = nn.Linear(1024, 1024)\n",
        "        self.OutputLayer = nn.Linear(1024, 2)\n",
        "        \n",
        "    def forward_once(self, X):\n",
        "        output = nn.functional.relu(self.HiddenLayer_1(X))\n",
        "        output = nn.functional.relu(self.HiddenLayer_2(output))\n",
        "        output = self.OutputLayer(output)\n",
        "        return output\n",
        "    \n",
        "    def forward(self, X1, X2):\n",
        "        out_1 = self.forward_once(X1)\n",
        "        out_2 = self.forward_once(X2)\n",
        "        return out_1, out_2\n",
        "        \n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, margin = 5.0):\n",
        "      super(ContrastiveLoss, self).__init__()\n",
        "      self.margin = margin\n",
        "      self.eps = 1e-6\n",
        "        \n",
        "    def forward(self, out_1, out_2, Y):\n",
        "      euclidean_distance = nn.functional.pairwise_distance(out_1, out_2)\n",
        "      loss_contrastive = torch.mean((Y) * torch.pow(euclidean_distance, 2) + (1 - Y) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "      return loss_contrastive\n",
        "\n",
        "\n",
        "class Operators():\n",
        "    \n",
        "    def __init__(self, net):\n",
        "      self.net = net\n",
        "      self.loss = ContrastiveLoss()\n",
        "      self.optimizer = torch.optim.SGD(self.net.parameters(), lr = 0.01)\n",
        "        \n",
        "    def train(self, data):      \n",
        "        for epoch in range(5000):\n",
        "          input_1, input_2, out = data.generate_batch()\n",
        "          X_1 = Variable(torch.Tensor(input_1).float())\n",
        "          X_2 = Variable(torch.Tensor(input_2).float())\n",
        "          Y = Variable(torch.Tensor(out).float())\n",
        "          self.optimizer.zero_grad()\n",
        "          out_1, out_2 = self.net.forward(X_1, X_2)\n",
        "          loss_val = self.loss.forward(out_1, out_2, Y)\n",
        "          loss_val.backward()\n",
        "          self.optimizer.step()\n",
        "          if epoch % 500 == 0:\n",
        "            print('Epoch: %d Loss: %.3f' % (epoch, loss_val))\n",
        "        \n",
        "    def test(self, dataX):\n",
        "        self.net.eval()\n",
        "        val_loss = 0\n",
        "        for epoch in range(5000):\n",
        "          input_1, input_2, out = data.generate_batch_test()\n",
        "          X_1 = Variable(torch.Tensor(input_1).float())\n",
        "          X_2 = Variable(torch.Tensor(input_2).float())\n",
        "          Y = Variable(torch.Tensor(out).float())\n",
        "          self.optimizer.zero_grad()\n",
        "          out_1, out_2 = self.net.forward(X_1, X_2)\n",
        "          loss_val = self.loss.forward(out_1, out_2, Y)\n",
        "          loss_val.backward()\n",
        "          self.optimizer.step()\n",
        "          val_loss += loss_val\n",
        "        val_loss /= 5000\n",
        "        print('Test Loss: %.3f' % (val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNOzJDzsZjJX"
      },
      "source": [
        "net = MODEL()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30LWq1jcag-O"
      },
      "source": [
        "data = DATA()\n",
        "data.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzTemZWAdq2l"
      },
      "source": [
        "modeloperator = Operators(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITobmhwpd1z1",
        "outputId": "346fe854-0276-4f55-cd4b-2baa5706a86c"
      },
      "source": [
        "modeloperator.train(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 21.979\n",
            "Epoch: 500 Loss: 1.297\n",
            "Epoch: 1000 Loss: 1.204\n",
            "Epoch: 1500 Loss: 1.301\n",
            "Epoch: 2000 Loss: 1.160\n",
            "Epoch: 2500 Loss: 0.669\n",
            "Epoch: 3000 Loss: 0.691\n",
            "Epoch: 3500 Loss: 0.553\n",
            "Epoch: 4000 Loss: 1.293\n",
            "Epoch: 4500 Loss: 1.092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Du-kK9uKqhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bd854e-3939-4114-d8c5-42d51d11b4dc"
      },
      "source": [
        "modeloperator.test(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7POQv1Ay7i8i"
      },
      "source": [
        "# Siamese Network Using Triplet Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l19tDb960QG"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TripletMNIST(Dataset):\n",
        "    def __init__(self, mnist_dataset):\n",
        "        self.mnist_dataset = mnist_dataset\n",
        "        self.train = self.mnist_dataset.train\n",
        "        self.transform = self.mnist_dataset.transform\n",
        "\n",
        "        if self.train:\n",
        "            self.train_labels = self.mnist_dataset.train_labels\n",
        "            self.train_data = self.mnist_dataset.train_data\n",
        "            self.labels_set = set(self.train_labels.numpy())\n",
        "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "        else:\n",
        "            self.test_labels = self.mnist_dataset.test_labels\n",
        "            self.test_data = self.mnist_dataset.test_data\n",
        "            # generate fixed triplets for testing\n",
        "            self.labels_set = set(self.test_labels.numpy())\n",
        "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
        "                                     for label in self.labels_set}\n",
        "\n",
        "            random_state = np.random.RandomState(29)\n",
        "\n",
        "            triplets = [[i,\n",
        "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
        "                         random_state.choice(self.label_to_indices[\n",
        "                                                 np.random.choice(\n",
        "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
        "                                                 )\n",
        "                                             ])\n",
        "                         ]\n",
        "                        for i in range(len(self.test_data))]\n",
        "            self.test_triplets = triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
        "            positive_index = index\n",
        "            while positive_index == index:\n",
        "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
        "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
        "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
        "            img2 = self.train_data[positive_index]\n",
        "            img3 = self.train_data[negative_index]\n",
        "        else:\n",
        "            img1 = self.test_data[self.test_triplets[index][0]]\n",
        "            img2 = self.test_data[self.test_triplets[index][1]]\n",
        "            img3 = self.test_data[self.test_triplets[index][2]]\n",
        "\n",
        "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
        "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
        "        img3 = Image.fromarray(img3.numpy(), mode='L')\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "        return (img1, img2, img3), []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZF0jmyA7oK3"
      },
      "source": [
        "class TripletNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embedding_net = nn.Sequential(nn.Linear(28*28, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 2)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        output1 = self.embedding_net(x1)\n",
        "        output2 = self.embedding_net(x2)\n",
        "        output3 = self.embedding_net(x3)\n",
        "        return output1, output2, output3\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.embedding_net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5xVfa6r76jl",
        "outputId": "7cfdda19-5c74-4448-9973-97bc7be90f52"
      },
      "source": [
        "trans = transforms.Compose([transforms.ToTensor()])\n",
        "data_loader = TripletMNIST(torchvision.datasets.MNIST(root=\"~/torch_datasets\", train = True, transform = trans, download=True))\n",
        "val_loader = TripletMNIST(torchvision.datasets.MNIST(root=\"~/torch_datasets\", train = False, transform = trans, download=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:54: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:64: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:59: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:69: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbT4NLGV9QMh",
        "outputId": "00046bf2-5306-4826-b693-f3a9fa5bdaec"
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "model = TripletNet()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  running_loss = 0\n",
        "  for data in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    i , images = data\n",
        "    imagesList = list(images[0]) \n",
        "    imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "    imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "    imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "    images = tuple(imagesList)\n",
        "    output = model(*images)\n",
        "    loss = triplet_loss(*output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  running_loss/= len(data_loader)\n",
        "  print('\\nTrain set: Average loss: {:.4f} , epoch: {}'.format(running_loss,epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set: Average loss: 0.3311 , epoch: 0\n",
            "\n",
            "Train set: Average loss: 0.2199 , epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn2XV1Wn27J0",
        "outputId": "4709a1cf-3672-46f0-be89-9b6ab0669df7"
      },
      "source": [
        " with torch.no_grad():\n",
        "   model.eval()\n",
        "   val_loss = 0\n",
        "   for batch_idx, (data, target) in enumerate(val_loader):\n",
        "     imagesList = list(data)\n",
        "     imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "     imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "     imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "     data = tuple(imagesList)\n",
        "     outputs = model(*data)\n",
        "     loss = triplet_loss(*output)\n",
        "     val_loss+=loss.item()\n",
        "   val_loss/=len(val_loader)\n",
        "   print('test set: Average loss: {:.4f} '.format(running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\test set: Average loss: 0.2199 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iJvNrR54U4C"
      },
      "source": [
        "# Using RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj--toeR4j1J",
        "outputId": "ac6a4fa2-45a1-4d0a-f74e-d6808a68ec3c"
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "model = TripletNet()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  running_loss = 0\n",
        "  for data in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    i , images = data\n",
        "    imagesList = list(images[0]) \n",
        "    imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "    imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "    imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "    images = tuple(imagesList)\n",
        "    output = model(*images)\n",
        "    loss = triplet_loss(*output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  running_loss/= len(data_loader)\n",
        "  print('\\nTrain set: Average loss: {:.4f} , epoch: {}'.format(running_loss,epoch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set: Average loss: 41.7002 , epoch: 0\n",
            "\n",
            "Train set: Average loss: 82.6408 , epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob0Ff1DP4pJt",
        "outputId": "56f7fbe6-2141-4c00-c14f-1050fdfcc509"
      },
      "source": [
        " with torch.no_grad():\n",
        "   model.eval()\n",
        "   val_loss = 0\n",
        "   for batch_idx, (data, target) in enumerate(val_loader):\n",
        "     imagesList = list(data)\n",
        "     imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "     imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "     imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "     data = tuple(imagesList)\n",
        "     outputs = model(*data)\n",
        "     loss = triplet_loss(*output)\n",
        "     val_loss+=loss.item()\n",
        "   val_loss/=len(val_loader)\n",
        "   print('test set: Average loss: {:.4f} '.format(running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set: Average loss: 82.6408 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7s6D05JWYl"
      },
      "source": [
        "# Using Mini Batch gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXEC_UzIJguZ",
        "outputId": "fd70d891-f8ff-4557-c5a0-e38b3967bfae"
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "model = TripletNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  running_loss = 0\n",
        "  for data in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    i , images = data\n",
        "    imagesList = list(images[0]) \n",
        "    imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "    imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "    imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "    images = tuple(imagesList)\n",
        "    output = model(*images)\n",
        "    loss = triplet_loss(*output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  running_loss/= len(data_loader)\n",
        "  print('\\nTrain set: Average loss: {:.4f} , epoch: {}'.format(running_loss,epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set: Average loss: 0.1925 , epoch: 0\n",
            "\n",
            "Train set: Average loss: 0.1176 , epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrfyBAVRJj48",
        "outputId": "4bedc269-a2f6-4f82-8086-28630e3d35cd"
      },
      "source": [
        " with torch.no_grad():\n",
        "   model.eval()\n",
        "   val_loss = 0\n",
        "   for batch_idx, (data, target) in enumerate(val_loader):\n",
        "     imagesList = list(data)\n",
        "     imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "     imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "     imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "     data = tuple(imagesList)\n",
        "     outputs = model(*data)\n",
        "     loss = triplet_loss(*output)\n",
        "     val_loss+=loss.item()\n",
        "   val_loss/=len(val_loader)\n",
        "   print('test set: Average loss: {:.4f} '.format(running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set: Average loss: 0.1176 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4pM6bAtMiGQ"
      },
      "source": [
        "# changing the margin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQO8VUzMn0e",
        "outputId": "5aa296dc-100f-4fd2-8872-0e7f1b55a62b"
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=2.0, p=2)\n",
        "model = TripletNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  running_loss = 0\n",
        "  for data in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    i , images = data\n",
        "    imagesList = list(images[0]) \n",
        "    imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "    imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "    imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "    images = tuple(imagesList)\n",
        "    output = model(*images)\n",
        "    loss = triplet_loss(*output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  running_loss/= len(data_loader)\n",
        "  print('\\nTrain set: Average loss: {:.4f} , epoch: {}'.format(running_loss,epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set: Average loss: 0.4423 , epoch: 0\n",
            "\n",
            "Train set: Average loss: 0.3482 , epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBOPYQt5Mru8",
        "outputId": "0ef2e677-7a79-433b-ae52-aa40bb96d109"
      },
      "source": [
        " with torch.no_grad():\n",
        "   model.eval()\n",
        "   val_loss = 0\n",
        "   for batch_idx, (data, target) in enumerate(val_loader):\n",
        "     imagesList = list(data)\n",
        "     imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "     imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "     imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "     data = tuple(imagesList)\n",
        "     outputs = model(*data)\n",
        "     loss = triplet_loss(*output)\n",
        "     val_loss+=loss.item()\n",
        "   val_loss/=len(val_loader)\n",
        "   print('test set: Average loss: {:.4f} '.format(running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set: Average loss: 0.3482 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpDTphU-M1Fj"
      },
      "source": [
        "#changing the p value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5pSi__eM8oP",
        "outputId": "41a44e68-d840-4741-8324-c55ed4a8bba8"
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=1)\n",
        "model = TripletNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "  running_loss = 0\n",
        "  for data in enumerate(data_loader):\n",
        "    optimizer.zero_grad()\n",
        "    i , images = data\n",
        "    imagesList = list(images[0]) \n",
        "    imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "    imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "    imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "    images = tuple(imagesList)\n",
        "    output = model(*images)\n",
        "    loss = triplet_loss(*output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  running_loss/= len(data_loader)\n",
        "  print('\\nTrain set: Average loss: {:.4f} , epoch: {}'.format(running_loss,epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set: Average loss: 0.2361 , epoch: 0\n",
            "\n",
            "Train set: Average loss: 0.1957 , epoch: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_hCb7yEM_22",
        "outputId": "809e060f-8f57-4b5f-f8f7-ef616c2f4b0c"
      },
      "source": [
        " with torch.no_grad():\n",
        "   model.eval()\n",
        "   val_loss = 0\n",
        "   for batch_idx, (data, target) in enumerate(val_loader):\n",
        "     imagesList = list(data)\n",
        "     imagesList[0] = imagesList[0].reshape(-1,784)\n",
        "     imagesList[1] = imagesList[1].reshape(-1,784)\n",
        "     imagesList[2] = imagesList[2].reshape(-1,784)\n",
        "     data = tuple(imagesList)\n",
        "     outputs = model(*data)\n",
        "     loss = triplet_loss(*output)\n",
        "     val_loss+=loss.item()\n",
        "   val_loss/=len(val_loader)\n",
        "   print('test set: Average loss: {:.4f} '.format(running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set: Average loss: 0.1957 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMOSIjnVQROQ"
      },
      "source": [
        "# Pros of Siamese Network\n",
        "## More Robust to class Imbalance\n",
        "## Nice to an ensemble with the best classifier\n",
        "##Learning from Semantic Similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAiwaG44QkNV"
      },
      "source": [
        "# Cons of Siamese Network\n",
        "## Needs more training time than normal networks\n",
        "## Doesnâ€™t output probabilities"
      ]
    }
  ]
}